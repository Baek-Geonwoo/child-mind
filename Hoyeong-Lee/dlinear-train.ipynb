{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fef478b",
   "metadata": {
    "papermill": {
     "duration": 0.005461,
     "end_time": "2023-12-03T14:04:51.746198",
     "exception": false,
     "start_time": "2023-12-03T14:04:51.740737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae529b82",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:51.759818Z",
     "iopub.status.busy": "2023-12-03T14:04:51.759401Z",
     "iopub.status.idle": "2023-12-03T14:04:58.490036Z",
     "shell.execute_reply": "2023-12-03T14:04:58.489039Z"
    },
    "papermill": {
     "duration": 6.740282,
     "end_time": "2023-12-03T14:04:58.492398",
     "exception": false,
     "start_time": "2023-12-03T14:04:51.752116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from math import pi, sqrt, exp\n",
    "import sklearn,sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn,Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa \n",
    "import ctypes\n",
    "\n",
    "def normalize(y):\n",
    "    mean = y[:,0].mean().item()\n",
    "    std = y[:,0].std().item()\n",
    "    y[:,0] = (y[:,0]-mean)/(std+1e-16)\n",
    "    mean = y[:,1].mean().item()\n",
    "    std = y[:,1].std().item()\n",
    "    y[:,1] = (y[:,1]-mean)/(std+1e-16)\n",
    "    return y\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44aed53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.506245Z",
     "iopub.status.busy": "2023-12-03T14:04:58.505554Z",
     "iopub.status.idle": "2023-12-03T14:04:58.510377Z",
     "shell.execute_reply": "2023-12-03T14:04:58.509588Z"
    },
    "papermill": {
     "duration": 0.01377,
     "end_time": "2023-12-03T14:04:58.512264",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.498494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "WARMUP_PROP = 0.2\n",
    "BS = 16\n",
    "WORKERS = 4\n",
    "TRAIN_PROP = 0.9\n",
    "max_chunk_size = 150000\n",
    "if device=='cpu':\n",
    "    torch.set_num_interop_threads(WORKERS)\n",
    "    torch.set_num_threads(WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6f7719",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.525165Z",
     "iopub.status.busy": "2023-12-03T14:04:58.524907Z",
     "iopub.status.idle": "2023-12-03T14:04:58.532699Z",
     "shell.execute_reply": "2023-12-03T14:04:58.531980Z"
    },
    "papermill": {
     "duration": 0.016567,
     "end_time": "2023-12-03T14:04:58.534605",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.518038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history, model_path=\".\", show=True):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Training Loss\")\n",
    "    plt.plot(epochs, history[\"valid_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss evolution\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(model_path, \"loss_evo.png\"))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(epochs, history[\"valid_mAP\"])\n",
    "#     plt.title(\"Validation mAP evolution\")\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(\"mAP\")\n",
    "#     plt.savefig(os.path.join(model_path, \"mAP_evo.png\"))\n",
    "#     if show:\n",
    "#         plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"lr\"])\n",
    "    plt.title(\"Learning Rate evolution\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"LR\")\n",
    "    plt.savefig(os.path.join(model_path, \"lr_evo.png\"))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213e6d0",
   "metadata": {
    "papermill": {
     "duration": 0.005618,
     "end_time": "2023-12-03T14:04:58.546202",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.540584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1725a586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.559209Z",
     "iopub.status.busy": "2023-12-03T14:04:58.558907Z",
     "iopub.status.idle": "2023-12-03T14:04:58.564818Z",
     "shell.execute_reply": "2023-12-03T14:04:58.564012Z"
    },
    "papermill": {
     "duration": 0.014683,
     "end_time": "2023-12-03T14:04:58.566601",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.551918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SIGMA = 720 #average length of day is 24*60*12 = 17280 for comparison\n",
    "# SAMPLE_FREQ = 12 # 1 obs per minute\n",
    "# class SleepDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         file\n",
    "#     ):\n",
    "#         self.targets,self.data,self.ids = joblib.load(file)\n",
    "            \n",
    "#     def downsample_seq_generate_features(self,feat, downsample_factor = SAMPLE_FREQ):\n",
    "#         # downsample data and generate features\n",
    "#         if len(feat)%SAMPLE_FREQ!=0:\n",
    "#             feat = np.concatenate([feat,np.zeros(SAMPLE_FREQ-((len(feat))%SAMPLE_FREQ))+feat[-1]])\n",
    "#         feat = np.reshape(feat, (-1,SAMPLE_FREQ))\n",
    "#         feat_mean = np.mean(feat,1)\n",
    "#         feat_std = np.std(feat,1)\n",
    "#         feat_median = np.median(feat,1)\n",
    "#         feat_max = np.max(feat,1)\n",
    "#         feat_min = np.min(feat,1)\n",
    "\n",
    "#         return np.dstack([feat_mean,feat_std,feat_median,feat_max,feat_min])[0]\n",
    "#     def downsample_seq(self,feat, downsample_factor = SAMPLE_FREQ):\n",
    "#         # downsample data\n",
    "#         if len(feat)%SAMPLE_FREQ!=0:\n",
    "#             feat = np.concatenate([feat,np.zeros(SAMPLE_FREQ-((len(feat))%SAMPLE_FREQ))+feat[-1]])\n",
    "#         feat = np.reshape(feat, (-1,SAMPLE_FREQ))\n",
    "#         feat_mean = np.mean(feat,1)\n",
    "#         return feat_mean\n",
    "    \n",
    "#     def gauss(self,n=SIGMA,sigma=SIGMA*0.15):\n",
    "#         # guassian distribution function\n",
    "#         r = range(-int(n/2),int(n/2)+1)\n",
    "#         return [1 / (sigma * sqrt(2*pi)) * exp(-float(x)**2/(2*sigma**2)) for x in r]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.targets)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         X = self.data[index][['anglez','enmo']]\n",
    "#         y = self.targets[index]\n",
    "\n",
    "#         # turn target inds into array\n",
    "#         target_guassian = np.zeros((len(X),2))\n",
    "#         for s,e in y:\n",
    "#             st1,st2 = max(0,s-SIGMA//2),s+SIGMA//2+1\n",
    "#             ed1,ed2 = e-SIGMA//2,min(len(X),e+SIGMA//2+1)\n",
    "#             target_guassian[st1:st2,0] = self.gauss()[st1-(s-SIGMA//2):]\n",
    "#             target_guassian[ed1:ed2,1] = self.gauss()[:SIGMA+1-((e+SIGMA//2+1)-ed2)]\n",
    "#             gc.collect()\n",
    "#         y = target_guassian\n",
    "#         gc.collect()\n",
    "#         X = np.concatenate([self.downsample_seq_generate_features(X.values[:,i],SAMPLE_FREQ) for i in range(X.shape[1])],-1)\n",
    "#         gc.collect()\n",
    "#         y = np.dstack([self.downsample_seq(y[:,i],SAMPLE_FREQ) for i in range(y.shape[1])])[0]\n",
    "#         gc.collect()\n",
    "#         y = normalize(y)\n",
    "#         #y = normalize(torch.from_numpy(y))\n",
    "#         #X = torch.from_numpy(X)\n",
    "        \n",
    "#         result = np.concatenate((X, y), axis=1)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "    \n",
    "# train_ds = SleepDataset('/kaggle/input/sleep-critical-point-prepare-data/train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07dd6e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.579288Z",
     "iopub.status.busy": "2023-12-03T14:04:58.578785Z",
     "iopub.status.idle": "2023-12-03T14:04:58.582896Z",
     "shell.execute_reply": "2023-12-03T14:04:58.582079Z"
    },
    "papermill": {
     "duration": 0.012508,
     "end_time": "2023-12-03T14:04:58.584783",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.572275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_slide(train_ds, window_size, forcast_size):\n",
    "    data_list = []\n",
    "    dap_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(train_ds))):\n",
    "        X = train_ds[i]\n",
    "\n",
    "        for idx in range(0, len(X)-window_size-forcast_size):\n",
    "            data_list.append(X[idx : idx+window_size])\n",
    "            dap_list.append(X[idx+window_size : idx+window_size+forcast_size, -2:])\n",
    "            \n",
    "    return np.array(data_list, dtype='float16'), np.array(dap_list, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb9be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.597294Z",
     "iopub.status.busy": "2023-12-03T14:04:58.597035Z",
     "iopub.status.idle": "2023-12-03T14:04:58.600864Z",
     "shell.execute_reply": "2023-12-03T14:04:58.600096Z"
    },
    "papermill": {
     "duration": 0.012249,
     "end_time": "2023-12-03T14:04:58.602776",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.590527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = time_slide(train_ds, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356ee5b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.615308Z",
     "iopub.status.busy": "2023-12-03T14:04:58.615056Z",
     "iopub.status.idle": "2023-12-03T14:04:58.618513Z",
     "shell.execute_reply": "2023-12-03T14:04:58.617685Z"
    },
    "papermill": {
     "duration": 0.012043,
     "end_time": "2023-12-03T14:04:58.620526",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.608483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump((results), 'time_slide_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce2a79",
   "metadata": {
    "papermill": {
     "duration": 0.005468,
     "end_time": "2023-12-03T14:04:58.631653",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.626185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62412aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:04:58.644556Z",
     "iopub.status.busy": "2023-12-03T14:04:58.644248Z",
     "iopub.status.idle": "2023-12-03T14:05:45.038009Z",
     "shell.execute_reply": "2023-12-03T14:05:45.037192Z"
    },
    "papermill": {
     "duration": 46.402939,
     "end_time": "2023-12-03T14:05:45.040241",
     "exception": false,
     "start_time": "2023-12-03T14:04:58.637302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = joblib.load('/kaggle/input/time-slide/time_slide_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24eb7523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.053751Z",
     "iopub.status.busy": "2023-12-03T14:05:45.053464Z",
     "iopub.status.idle": "2023-12-03T14:05:45.059347Z",
     "shell.execute_reply": "2023-12-03T14:05:45.058512Z"
    },
    "papermill": {
     "duration": 0.014667,
     "end_time": "2023-12-03T14:05:45.061200",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.046533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398885e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.074052Z",
     "iopub.status.busy": "2023-12-03T14:05:45.073768Z",
     "iopub.status.idle": "2023-12-03T14:05:45.092711Z",
     "shell.execute_reply": "2023-12-03T14:05:45.091872Z"
    },
    "papermill": {
     "duration": 0.027667,
     "end_time": "2023-12-03T14:05:45.094649",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.066982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        residual = x - moving_mean\n",
    "        return moving_mean, residual \n",
    "    \n",
    "class LTSF_DLinear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, kernel_size, individual, feature_size):\n",
    "        super(LTSF_DLinear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = torch.nn.ModuleList()\n",
    "            self.Linear_Trend = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Trend.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Trend[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "                self.Linear_Seasonal.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Seasonal[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "        else:\n",
    "            self.Linear_Trend = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "            self.Linear_Trend.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "            self.Linear_Seasonal = torch.nn.Linear(self.window_size,  self.forcast_size)\n",
    "            self.Linear_Seasonal.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        trend_init, seasonal_init = self.decompsition(x)\n",
    "        trend_init, seasonal_init = trend_init.permute(0,2,1), seasonal_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.forcast_size], dtype=trend_init.dtype).to(trend_init.device)\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.forcast_size], dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            for idx in range(self.channels):\n",
    "                trend_output[:, idx, :] = self.Linear_Trend[idx](trend_init[:, idx, :])\n",
    "                seasonal_output[:, idx, :] = self.Linear_Seasonal[idx](seasonal_init[:, idx, :])                \n",
    "        else:\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x[:, :, -2:]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbc020",
   "metadata": {
    "papermill": {
     "duration": 0.005495,
     "end_time": "2023-12-03T14:05:45.105843",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.100348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5849d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.118358Z",
     "iopub.status.busy": "2023-12-03T14:05:45.118075Z",
     "iopub.status.idle": "2023-12-03T14:05:45.128361Z",
     "shell.execute_reply": "2023-12-03T14:05:45.127533Z"
    },
    "papermill": {
     "duration": 0.018813,
     "end_time": "2023-12-03T14:05:45.130295",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.111482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, device, scheduler):\n",
    "    #model = nn.DataParallel(model, device_ids=[0, 1], output_device=0)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    best_score = 0\n",
    "    max_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    for epoch in range(1, 5):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, Y in tqdm(iter(val_loader)):\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                output = model(X)    \n",
    "                loss = criterion(output, Y)\n",
    "                valid_loss.append(loss.item())\n",
    "                \n",
    "        valid_loss_list.append(np.mean(valid_loss))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if loss < max_loss:\n",
    "            torch.save(model, './DLinear_model.pth')\n",
    "            max_loss = loss\n",
    "            \n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{np.mean(valid_loss):.5f}]')\n",
    "    \n",
    "    return model, train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01cc2306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.142940Z",
     "iopub.status.busy": "2023-12-03T14:05:45.142673Z",
     "iopub.status.idle": "2023-12-03T14:05:45.146556Z",
     "shell.execute_reply": "2023-12-03T14:05:45.145792Z"
    },
    "papermill": {
     "duration": 0.012123,
     "end_time": "2023-12-03T14:05:45.148324",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.136201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input = results[0]\n",
    "train_target = results[1]\n",
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52258584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.160924Z",
     "iopub.status.busy": "2023-12-03T14:05:45.160650Z",
     "iopub.status.idle": "2023-12-03T14:05:45.419982Z",
     "shell.execute_reply": "2023-12-03T14:05:45.419130Z"
    },
    "papermill": {
     "duration": 0.268231,
     "end_time": "2023-12-03T14:05:45.422288",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.154057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input_ids = train_input[:, 0, 0].astype(int)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571db22b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.436324Z",
     "iopub.status.busy": "2023-12-03T14:05:45.435363Z",
     "iopub.status.idle": "2023-12-03T14:05:45.439785Z",
     "shell.execute_reply": "2023-12-03T14:05:45.438952Z"
    },
    "papermill": {
     "duration": 0.01341,
     "end_time": "2023-12-03T14:05:45.441755",
     "exception": false,
     "start_time": "2023-12-03T14:05:45.428345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14440792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T14:05:45.454670Z",
     "iopub.status.busy": "2023-12-03T14:05:45.454378Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-12-03T14:05:45.447508",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fcb991b2ec44b3ba27f1f2c15c0854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/599126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca4b186329e4f17896de1e50cee1fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [nan] Val Loss : [nan]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e2a63bc5d24691a259c47717f39850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/599126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b844326ae944b3aee48d0e3d5fd753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [nan] Val Loss : [nan]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac19fceef39a4501b1452003d466533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/599126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(train_input, train_input_ids)):\n",
    "    print(f'fold: {fold_num}', '='*50)\n",
    "    \n",
    "    train_input_data, train_target_data = train_input[train_idx], train_target[train_idx]\n",
    "    val_input_data, val_target_data = train_input[valid_idx], train_target[valid_idx]\n",
    "\n",
    "    train_dataset = CustomDataset(train_input_data, train_target_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    val_dataset = CustomDataset(val_input_data, val_target_data)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 16, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    model = LTSF_DLinear(window_size=30, forcast_size=10, kernel_size=25, individual=False, feature_size=10)\n",
    "\n",
    "    scheduler = None\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    infer_model, train_loss_list, valid_loss_list = train(model, train_loader, val_loader, device, scheduler)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss_list, label='Training Loss')\n",
    "    plt.plot(valid_loss_list, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    image_file_path = f'./losses_plot_fold_{fold_num}.png'\n",
    "    plt.savefig(image_file_path)\n",
    "\n",
    "    del train_input_data, train_target_data, val_input, val_target, train_dataset, train_loader\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6589269,
     "sourceId": 53666,
     "sourceType": "competition"
    },
    {
     "datasetId": 4100595,
     "sourceId": 7111563,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146367017,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-03T14:04:48.355736",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
